{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on GPU: NVIDIA GeForce RTX 2080 SUPER\n",
    "\n",
    "--> For training cfg/yolov4-custom.cfg download the pre-trained weights-file (162 MB): yolov4.conv.137 (Google      drive mirror yolov4.conv.137 )\n",
    "    ??? Why not the original weight file which was used for inferencing ???\n",
    "    \n",
    "--> Create file yolo-obj.cfg with the same content as in yolov4-custom.cfg:\n",
    "    * change line batch to batch=64\n",
    "    * change line subdivisions to subdivisions=16\n",
    "    * change line max_batches to (classes*2000 but not less than number of training images, but not less than number of training images and not less than 6000), f.e. max_batches=6000 if you train for 3 classes\n",
    "    * change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400\n",
    "    * set network size width=416 height=416 or any value multiple of 32:\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L8-L9\n",
    "    * change line classes=80 to your number of objects in each of 3 [yolo]-layers:\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L610\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L696\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L783\n",
    "    * change [filters=255] to filters=(classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers.\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L603\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L689\n",
    "      - https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L776\n",
    "    * when using [Gaussian_yolo] layers, change [filters=57] filters=(classes + 9)x3 in the 3 [convolutional] before each [Gaussian_yolo] layer\n",
    "      - https://github.com/AlexeyAB/darknet/blob/6e5bdf1282ad6b06ed0e962c3f5be67cf63d96dc/cfg/Gaussian_yolov3_BDD.cfg#L604\n",
    "      - https://github.com/AlexeyAB/darknet/blob/6e5bdf1282ad6b06ed0e962c3f5be67cf63d96dc/cfg/Gaussian_yolov3_BDD.cfg#L696\n",
    "      - https://github.com/AlexeyAB/darknet/blob/6e5bdf1282ad6b06ed0e962c3f5be67cf63d96dc/cfg/Gaussian_yolov3_BDD.cfg#L789\n",
    "      - This is probably not applicable\n",
    "    * So if classes=1 then should be filters=18. If classes=2 then write filters=21\n",
    "        (Do not write in the cfg-file: filters=(classes + 5)x3)\n",
    "        (Generally filters depends on the classes, coords and number of masks, i.e. filters=(classes + coords + 1)*<number of mask>, where mask is indices of anchors. If mask is absence, then filters=(classes + coords + 1)*num)\n",
    "        So for example, for 2 objects, your file yolo-obj.cfg should differ from yolov4-custom.cfg in such lines in each of 3 [yolo]-layers:\n",
    "\n",
    "            [convolutional]\n",
    "            filters=21\n",
    "\n",
    "            [region]\n",
    "            classes=2\n",
    "            \n",
    "--> Create file obj.names in the directory build\\darknet\\x64\\data\\, with objects names - each in new line\n",
    "--> Create file obj.data in the directory build\\darknet\\x64\\data\\, containing (where classes = number of objects):\n",
    "            classes = 2\n",
    "            train  = data/train.txt\n",
    "            valid  = data/test.txt\n",
    "            names = data/obj.names\n",
    "            backup = backup/\n",
    "\n",
    "--> Put image-files (.jpg) of your objects in the directory build\\darknet\\x64\\data\\obj\\\n",
    "--> You should label each object on images from your dataset.\n",
    "        It will create .txt-file for each .jpg-image-file - in the same directory and with the same name, but with .txt-extension, and put to file: object number and object coordinates on this image, for each object in new line:\n",
    "\n",
    "            <object-class> <x_center> <y_center> <width> <height>\n",
    "\n",
    "        Where:\n",
    "\n",
    "            <object-class> - integer object number from 0 to (classes-1)\n",
    "            <x_center> <y_center> <width> <height> - float values relative to width and height of image, it can be equal from (0.0 to 1.0]\n",
    "            for example: <x> = <absolute_x> / <image_width> or <height> = <absolute_height> / <image_height>\n",
    "            atention: <x_center> <y_center> - are center of rectangle (are not top-left corner)\n",
    "\n",
    "--> Create file train.txt in directory build\\darknet\\x64\\data\\, with filenames of your images, each filename in new line, with path relative to darknet.exe, for example containing:\n",
    "        data/obj/img1.jpg\n",
    "        data/obj/img2.jpg\n",
    "        data/obj/img3.jpg\n",
    "        \n",
    "        \n",
    "--> Download pre-trained weights for the convolutional layers and put to the directory build\\darknet\\x64\n",
    "        for yolov4.cfg, yolov4-custom.cfg (162 MB): yolov4.conv.137 (Google drive mirror yolov4.conv.137 )\n",
    "        \n",
    "--> Start training by using the command line: darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137\n",
    "         use ./darknet instead of darknet.exe for using it in Linux or MacOS\n",
    "    - file yolo-obj_last.weights will be saved to the build\\darknet\\x64\\backup\\ for each 100 iterations\n",
    "    - file yolo-obj_xxxx.weights will be saved to the build\\darknet\\x64\\backup\\ for each 1000 iterations\n",
    "    - to disable Loss-Window use darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show, if you train on computer without monitor like a cloud Amazon EC2\n",
    "    - to see the mAP & Loss-chart during training on remote server without GUI, use command darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map then open URL http://ip-address:8090 in Chrome/Firefox browser\n",
    "    \n",
    "--> For training with mAP (mean average precisions) calculation for each 4 Epochs (set valid=valid.txt or train.txt in obj.data file) and run:      darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -map\n",
    "--> After training is complete - get result yolo-obj_final.weights from path build\\darknet\\x64\\backup\\\n",
    "--> After each 100 iterations you can stop and later start training from this point. For example, after 2000 iterations you can stop training,      and later just start training using: darknet.exe detector train data/obj.data yolo-obj.cfg backup\\yolo-obj_2000.weights\n",
    "--> Also you can get result earlier than all 45000 iterations\n",
    "\n",
    "    *    Note: If during training you see nan values for avg (loss) field - then training goes wrong, but if nan is in some other lines - then      training goes well.\n",
    "    *    Note: If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.\n",
    "    *   Note: After training use such command for detection: darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights\n",
    "    *    Note: if error Out of memory occurs then in .cfg-file you should increase subdivisions=16, 32 or 64: link\n",
    "\n",
    "\n",
    "\n",
    "How to improve object detection?\n",
    "--> Before training:\n",
    "    *   set flag random=1 in your .cfg-file - it will increase precision by training Yolo for different resolutions: link\n",
    "\n",
    "    *   increase network resolution in your .cfg-file (height=608, width=608 or any value multiple of 32) - it will increase precision\n",
    "    \n",
    "    *   check that each object that you want to detect is mandatory labeled in your dataset - no one object in your data set should not be         without label. In the most training issues - there are wrong labels in your dataset (got labels by using some conversion script,           marked with a third-party tool, ...). Always check your dataset by using: https://github.com/AlexeyAB/Yolo_mark\n",
    "\n",
    "    *   my Loss is very high and mAP is very low, is training wrong? Run training with -show_imgs flag at the end of training command, do you     see correct bounded boxes of objects (in windows or in files aug_...jpg)? If no - your training dataset is wrong.\n",
    "\n",
    "    *   for each object which you want to detect - there must be at least 1 similar object in the Training dataset with about the same: shape,     side of object, relative size, angle of rotation, tilt, illumination. So desirable that your training dataset include images with         objects at diffrent: scales, rotations, lightings, from different sides, on different backgrounds - you should preferably have 2000       different images for each class or more, and you should train 2000*classes iterations or more\n",
    "\n",
    "    *   desirable that your training dataset include images with non-labeled objects that you do not want to detect - negative samples without     bounded box (empty .txt files) - use as many images of negative samples as there are images with objects\n",
    "\n",
    "    *   What is the best way to mark objects: label only the visible part of the object, or label the visible and overlapped part of the           object, or label a little more than the entire object (with a little gap)? Mark as you like - how would you like it to be detected.\n",
    "\n",
    "    *   for training with a large number of objects in each image, add the parameter max=200 or higher value in the last [yolo]-layer or           [region]-layer in your cfg-file (the global maximum number of objects that can be detected by YoloV3 is 0,0615234375*(width*height)       where are width and height are parameters from [net] section in cfg-file)\n",
    "\n",
    "    *   for training for small objects (smaller than 16x16 after the image is resized to 416x416) - set layers = 23 instead of                     https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895\n",
    "\n",
    "        - set stride=4 instead of https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L892\n",
    "        - set stride=4 instead of https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L989\n",
    "\n",
    "    *   for training for both small and large objects use modified models:\n",
    "\n",
    "        - Full-model: 5 yolo layers: https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg\n",
    "        - Tiny-model: 3 yolo layers: https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny_3l.cfg\n",
    "        - YOLOv4: 3 yolo layers: https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-custom.cfg\n",
    "\n",
    "    *   If you train the model to distinguish Left and Right objects as separate classes (left/right hand, left/right-turn on road signs, ...)     then for disabling flip data augmentation - add flip=0 here:                                                                               https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17\n",
    "\n",
    "    *   General rule - your training dataset should include such a set of relative sizes of objects that you want to detect:\n",
    "\n",
    "        - train_network_width * train_obj_width / train_image_width ~= detection_network_width * detection_obj_width / detection_image_width\n",
    "        - train_network_height * train_obj_height / train_image_height ~= detection_network_height * detection_obj_height / detection_image_height\n",
    "        I.e. for each object from Test dataset there must be at least 1 object in the Training dataset with the same class_id and about the same relative size:\n",
    "\n",
    "            object width in percent from Training dataset ~= object width in percent from Test dataset\n",
    "\n",
    "        That is, if only objects that occupied 80-90% of the image were present in the training set, then the trained network will not be able to detect objects that occupy 1-10% of the image.\n",
    "\n",
    "    *   to speedup training (with decreasing detection accuracy) set param stopbackward=1 for layer-136 in cfg-file\n",
    "\n",
    "    *   each: model of object, side, illimination, scale, each 30 grad of the turn and inclination angles - these are different objects from       an internal perspective of the neural network. So the more different objects you want to detect, the more complex network model           should be used.\n",
    "\n",
    "    *   to make the detected bounded boxes more accurate, you can add 3 parameters ignore_thresh = .9 iou_normalizer=0.5 iou_loss=giou to each     [yolo] layer and train, it will increase mAP@0.9, but decrease mAP@0.5.\n",
    "\n",
    "    *   Only if you are an expert in neural detection networks - recalculate anchors for your dataset for width and height from cfg-file:         darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416 then set the same 9 anchors in each of 3         [yolo]-layers in your cfg-file. But you should change indexes of anchors masks= for each [yolo]-layer, so for YOLOv4 the 1st-[yolo]-       layer has anchors smaller than 30x30, 2nd smaller than 60x60, 3rd remaining, and vice versa for YOLOv3. Also you should change the         filters=(classes + 5)*<number of mask> before each [yolo]-layer. If many of the calculated anchors do not fit under the appropriate       layers - then just try using all the default anchors.\n",
    "\n",
    "--> After training - for detection:\n",
    "    *   Increase network-resolution by set in your .cfg-file (height=608 and width=608) or (height=832 and width=832) or (any value multiple       of 32) - this increases the precision and makes it possible to detect small objects: link\n",
    "\n",
    "    *   it is not necessary to train the network again, just use .weights-file already trained for 416x416 resolution\n",
    "\n",
    "    *   to get even greater accuracy you should train with higher resolution 608x608 or 832x832, note: if error Out of memory occurs then in       .cfg-file you should increase subdivisions=16, 32 or 64: link\n",
    "\n",
    "                \n",
    "## References:\n",
    "    https://github.com/AlexeyAB/darknet\n",
    "    https://github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-(trainvalno5k-2014)-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLOv4 using tflite\n",
    "https://github.com/hunglc007/tensorflow-yolov4-tflite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch YOLOv4\n",
    "    https://github.com/WongKinYiu/PyTorch_YOLOv4\n",
    "    https://github.com/maudzung/Complex-YOLOv4-Pytorch\n",
    "    https://github.com/Tianxiaomo/pytorch-YOLOv4#22-image-input-size-for-inference\n",
    "    https://github.com/WongKinYiu/PyTorch_YOLOv4\n",
    "    https://github.com/VCasecnikovs/Yet-Another-YOLOv4-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
